---
layout: post
comments: true
title:  "Introduction to Reinforcement Learning: Part 2 - Dynamic Programming"
excerpt: "The different Bellman Equations will be introduced. Afterwards we discuss Value and Policy Iteration and sketch proofs of those dynamic programming algorithms. "
date:   2017-08-10
mathjax: true
---


## Dynamic Programming

Dynamic Programming (DP) is an optimization approach that transforms complex problems into overlapping, simpler sub-problems. In our context, the term DP refers to a class of algorithms, which try to compute optimal policies in a known MDP, i.e. $<\mathcal{S},\mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma>$ is given. In later posts the assumption of a known environment will be dropped (model-free setting) - but even in those settings the ideas behind DP are still relevant.

We concentrate on infinite-horizon discounted MDPs. Throughout the rest of this post we assume that both the state as well as the action space are finite. Besides, $\gamma < 1$ and the reward function is bounded by $M \in \mathbb{R}$. One nice consequence of these assumptions is the existence of an optimal policy.

### Bellman Equations

The definitions of the different (optimal) value functions can be rewritten in a recursive manner, which gives us the famous Bellman (Optimality) Equations: Fix policy $\pi \in \Pi$, and an arbitrary state $s \in \mathcal{S}$:

$$ V_{\pi}(s) := \mathbb{E} _{\pi} [ \sum _{t=0}^{\infty} \gamma ^t R_t \mid s ] = \mathbb{E} _{\pi} [R_0 + \gamma \sum_{t=1} \gamma^{t-1} R_t \mid s] \\ = \mathcal{R} (s,\pi(s)) + \gamma \sum _{s' \in \mathcal{S}} \mathcal{P} (s, \pi (s), s') \mathbb{E} _{\pi} [ \sum _{t=0}^{\infty} \gamma ^t R_t \mid s' ] \\ = \mathcal{R} (s,\pi(s)) + \gamma \sum _{s' \in \mathcal{S}} \mathcal{P} (s, \pi (s), s') V _{\pi} (s') $$

Analogously, 

$$ Q_{\pi} (s,a) = \mathcal{R} (s,a) + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P} (s,a,s') Q_{\pi} (s', \pi (s')) $$

$$ Q^{\ast} (s,a) = \mathcal{R} (s,a) + \gamma \sum_{ s' \in \mathcal{S} } \mathcal{P} (s,a,s') V^{\ast} (s')  = \mathcal{R} (s,a) + \gamma \sum_{ s' \in \mathcal{S} } \mathcal{P} (s,a,s') sup_{a' \in \mathcal{A}} Q^{\ast} (s', a')$$

$$ V^{\ast} (s) = sup_{a \in \mathcal{A}} Q^{\ast} (s,a) = sup_{a \in \mathcal{A}} ( \mathcal{R} (s,a) + \gamma \sum_{ s' \in \mathcal{S} } \mathcal{P} (s,a,s') V^{\ast} (s') ) $$

It should be noted that $V_{\pi}$ can be interpreted as an element of $\mathbb{R}^{ \mid \mathcal{S} \mid}$, since $V_{\pi}: \mathcal{S} \rightarrow \mathbb{R}$ and $ \mid \mathcal{S} \mid < \infty$ (finite state space). Similarly, $Q_{\pi}$ can be seen as an element of $\mathbb{R}^{\mid \mathcal{S} \mid ^{ \mid \mathcal{A} \mid } }$ (use: $\mid \mathcal{A} \mid < \infty$). At the same time it has to be clear that not every element in $\mathbb{R}^{ \mid \mathcal{S} \mid}$ (or in $\mathbb{R}^{\mid \mathcal{S} \mid ^{ \mid \mathcal{A} \mid } } $) is related to a state-value function (or action-value function): consider that in our case there only exists a finite number of deterministic policies, hence only a finite number of different value functions. Any vector space over the real numbers is certainly not finite. 

Let us equip $\mathbb{R}^{ \mid \mathcal{S} \mid}$ and  $\mathbb{R}^{\mid \mathcal{S} \mid ^{ \mid \mathcal{A} \mid }} $ with the supremum norm (maximum norm) $ \mid \cdot \mid _{\infty}$. Due to the fact that all norms are equivalent on a finite dimensional vector space, we can still be sure that our considered spaces are complete. The Bellman Equations above can be used to define (not necessarily linear) operators on $\mathbb{R}^{ \mid \mathcal{S} \mid}$ and  $\mathbb{R}^{\mid \mathcal{S} \mid^{ \mid \mathcal{A} \mid } }$:

$$ T^{\pi}: \mathbb{R}^{ \mid \mathcal{S} \mid} \rightarrow \mathbb{R}^{ \mid \mathcal{S} \mid}$$, 
$$ V(s) \mapsto \mathcal{R} (s,\pi(s)) + \gamma \sum _{s' \in \mathcal{S}} \mathcal{P} (s, \pi (s), s') V _{\pi} (s') $$


$$ T^{\ast} : \mathbb{R}^{ \mid \mathcal{S} \mid} \rightarrow \mathbb{R}^{ \mid \mathcal{S} \mid} $$,
$$ V(s) \mapsto sup_{a \in \mathcal{A}} ( \mathcal{R} (s,a) + \gamma \sum_{ s' \in \mathcal{S} } \mathcal{P} (s,a,s') V^{\ast} (s') ) $$

$$ L^{\pi}: \mathbb{R}^{\mid \mathcal{S} \mid ^{ \mid \mathcal{A} \mid } } \rightarrow \mathbb{R}^{\mid \mathcal{S} \mid ^{ \mid \mathcal{A} \mid } } $$,
$$ Q(s,a) \mapsto \mathcal{R} (s,a) + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P} (s,a,s') Q (s', \pi (s')) $$

$$ L^{\ast}: \mathbb{R}^{\mid \mathcal{S} \mid ^{ \mid \mathcal{A} \mid } } \rightarrow \mathbb{R}^{\mid \mathcal{S} \mid ^{ \mid \mathcal{A} \mid } } $$
$$ Q(s,a) \mapsto \mathcal{R} (s,a) + \gamma \sum_{ s' \in \mathcal{S} } \mathcal{P} (s,a,s') sup_{a' \in \mathcal{A}} Q (s', a')$$

Due to the Bellman Equations we already know one fixed-point for every operator (e.g. $V_{\pi}$ is a fixed-point of $T^{\pi}$, and $V^{\ast}$ is a fixed-point of $T^{\ast}$). 

Even if the MDP is known, it is rather complicated to calculate value functions directly (main reason: size of MDP). As a consequence, one searches for iterative algorithms that converge to the desired value functions. Often this is called "evaluation". 
Let's assume we have a deterministic policy $\pi \in \Pi$ in mind and we want to calculate the corresponding state-value function $V_{\pi}$. As mentioned above, ($\mathbb{R}^{ \mid \mathcal{S} \mid}$, $ \mid \cdot \mid _{\infty}$ ) is complete and for $V, V' \in \mathbb{R}^{ \mid \mathcal{S} \mid}$

$$ \mid T^{\pi} (V) - T^{\pi} (V')  \mid_{\infty} = \mid \mathcal{R} (\cdot,\pi(\cdot)) + \gamma \sum _{s' \in \mathcal{S}} \mathcal{P} (\cdot, \pi (\cdot), s') V (s') -  \mathcal{R} (\cdot,\pi(\cdot)) - \gamma \sum _{s' \in \mathcal{S}} \mathcal{P} (\cdot, \pi (\cdot), s') V' (s') \mid _{\infty} \\ = \mid \gamma \sum _{s' \in \mathcal{S}} \mathcal{P} (\cdot, \pi (\cdot), s') (V (s') - V' (s'))\mid _{\infty} \leq \gamma \mid V (\cdot) - V' (\cdot) \mid _{\infty}$$,

i.e. $T^{\pi}$ is a contraction mapping (note: $\gamma$ < 1). Thus the Banach fixed-point theorem can be applied. As a consequence, $T^{\pi}$ admits a unique fixed-point. Furthermore, the sequence $(V_n) _{n \in \mathbb{N}} $ with $V _{n+1} := T^ {\pi} (V_n)$ and with an arbitrary element $V_1$ converges to the unique fixed-point. However, we already know that $V _{\pi}$ has to be this fixpoint! 
So, the Banach fixed-point theorem gives us an iterative algorithm that converges to the state-value function. 

Analogously, one can apply the Banach fixed-point theorem to the other operators $T^{\ast}$, $L^{\pi}$ and $L^{\ast}$ and in this way we obtain iterative algorithms for the calculation of $V^{\ast}$, $Q_{\pi}$ and $Q^{\ast}$.

### Value Iteration

Value iteration is just the name for the algorithm described above in the case of $T^{\ast}$ and $L^{\ast}$. Hence, value iteration is a method for finding an optimal policy via its value functions. 

*Reminder*: Acting greedily with respect to $V^{\ast}$ or $Q^{\ast}$ gives us an optimal policy, i.e. solution to the MDP (see: Part 1). In order to be able to act greedily wrt $V^{\ast}$, we have to make use of the Bellman equation, thus compute $argmax_{a \in \mathcal{A}} ( \mathcal{R} (s,a) + \gamma \sum_{ s' \in \mathcal{S} } \mathcal{P} (s,a,s') V^{\ast} (s') )$. This shows again the benefit of action-value functions. 

Let us summarize the value iteration algorithm one more time: we select an arbitrary $V_1 \in \mathbb{R}^{ \mid \mathcal{S} \mid}$ (resp.: $Q_1 \in \mathbb{R} ^{\mid \mathcal{S} \mid ^ { \mid \mathcal{A} \mid } }$) and then generate a sequence $V_{k+1} := T^{\ast} (V_k)$ (resp.: $Q_{k+1} := L^{\ast} (Q_k)$). Due to the argumentation above:

$$ V_1 \rightarrow V_2 \rightarrow V_3 \rightarrow ... \rightarrow V^{\ast} $$

$$ (Q_1 \rightarrow Q_2 \rightarrow Q_3 \rightarrow ... \rightarrow Q^{\ast}) $$  

Due to our limited amount of time and computational power, we will always be restricted to a finite number of iterative steps. However, the idea is that once $V_k$ (or: $Q_k$) is close to $V^{\ast}$ (or: $Q^{\ast}$), a policy that is greedy wrt $V_k$ (or: $Q_k$) will be close to an optimal policy. 
In the past researchers have shown that value iteration is rather a "theoretical" concept, since it has weak convergence properties in most of the real life applications. This brings us to the idea of policy iteration, which usually exhibits a superior performance. 


### Policy Iteration 
    
Comparable to value iteration, policy iteration is an iterative method that aims at solving a MDP with the help of state- or action-value functions. However, policy iteration switches constantly between policy evaluation and policy improvement. 
At iterative step $k$ a policy $\pi_k$ is given, which is then evaluated - i.e. we first compute $V_{\pi_k}$ (or: $Q_{\pi_k}$). For now, let us assume that the calculation of $V_{\pi_k}$ (or: $Q_{\pi_k}$) can be done easily. In later posts, we will discuss this part in more detail and think about asynchronous backups. After the evaluation, we improve the policy $\pi_k$ by acting greedily with respect to $V_{\pi_k}$ (or: $Q_{\pi_k}$). This results in a new policy $\pi_{k+1}$, and  brings us to iteration step $k+1$. Overall, this iterative alternation between evaluation and improvement, with an arbitrary initialization of $\pi_1$, converges to $\pi^{\ast}$ (optimal policy) and $V^{\ast}$ (or: $\pi^{\ast}$ (optimal policy) and $Q^{\ast}$):

 $$ \pi_{1} \rightarrow V_{\pi_{1}} \rightarrow \pi_{2} \rightarrow V_{2} \rightarrow \pi_{3} \rightarrow V_{3} \rightarrow ... \rightarrow V^{\ast} \rightarrow \pi^{\ast}$$
 
 $$ (\pi_{1} \rightarrow Q_{\pi_{1}} \rightarrow \pi_{2} \rightarrow Q_{2} \rightarrow \pi_{3} \rightarrow Q_{3} \rightarrow ... \rightarrow Q^{\ast} \rightarrow \pi^{\ast})$$

However, it is not directly clear why this procedure should really work and converge to our desired solution of the MDP. Due to the fact that the Q-function policy iteration is more important for us than the state-value version (reminder: benefits of action-value functions), we will sketch the proof of the action-value policy iteration here: as mentioned before, $\pi_0$ is initialized arbitrarily. Now, consider: for any iteration step $k$, any $s \in \mathcal{S}$, the improved policy is defined by $\pi_{k+1} (s) := argmax_{a \in \mathcal{A}} Q_{\pi_k} (s,a)$ (well-defined, since $\mid \mathcal{A} \mid < \infty$); thus $Q_{\pi_k} (s, \pi_{k+1} (s) ) = max_{a \in \mathcal{A}} Q_{\pi_k} (s,a) \geq Q_{\pi_k} (s, \pi_k (s)) = V_{\pi_k} (s)$. As a consequence, 

$$ V_{\pi_k} (s) \leq Q_{\pi_k} (s, \pi_{k+1}(s)) =  \mathcal{R} (s,\pi_{k+1}(s)) + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P} (s,\pi_{k+1}(s),s') Q_{\pi_k} (s', \pi_k (s')) \\ \leq \mathcal{R} (s,\pi_{k+1}) + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P} (s,\pi_{k+1}(s),s') Q_{\pi_k} (s', \pi_{k+1} (s')) \\ \leq ... \leq V_{\pi_{k+1}} (s)$$,

i.e. $V_{\pi_k} (s) \leq V_{\pi_{k+1}}(s)$ for $\forall s \in \mathcal{S}$. Thus, $\pi_k \leq \pi_{k+1}$ with respect to the partial ordering we defined in the last post. In other words, every greedy action really is an improvement of our given policy. 

Analogously, 

$$ Q_{\pi_k} (s,a) = \mathcal{R} (s,a) + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P} (s,a,s') Q_{\pi_k} (s', \pi_k (s')) \\ \leq \mathcal{R} (s,a) + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P} (s,a,s') Q_{\pi_k} (s', \pi_{k+1} (s')) \leq ... \leq Q_{\pi_{k+1}} (s,a) $$

Since $\gamma < 1$ and $\mid \mathcal{R} (s,a) \mid < M$ for $\forall s \in \mathcal{S}, \forall a \in \mathcal{A}$, the state-value functions, as well as the action-value functions, are bounded by some constant, e.g. $V_{\pi_k} (s) < M (1-\gamma)^{-1}$ for every $k$ (see last post). 

Thus $(Q_{\pi_k})_{k \in \mathbb{N}} \subseteq \mathbb{R}^{\mid \mathcal{S} \mid^{ \mid \mathcal{A} \mid } }$ (equipped with supremum norm) is a monotonically increasing, bounded sequence, thus converges to some $Q \in  \mathbb{R}^{\mid \mathcal{S} \mid ^{ \mid \mathcal{A} \mid } }$.  

This implies that $lim_{k \rightarrow \infty} Q_{\pi_k} (s, \pi_{k+1} (s) ) = lim_{k \rightarrow \infty}  max_{a \in \mathcal{A}} Q_{\pi_k} (s,a) = lim_{k \rightarrow \infty}  Q_{\pi_k} (s, \pi_k (s)) $ (otherwise we would still see improvements of the value function). Hence,    

  $$ Q(s,a) = lim_{k \rightarrow \infty} Q_{\pi_k} (s, a) =  lim_{k \rightarrow \infty} \mathcal{R} (s,a) + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P} (s,a,s') Q_{\pi_k} (s', \pi_k (s')) \\ = \mathcal{R} (s,a) + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P} (s,a,s') lim_{k \rightarrow \infty} Q_{\pi_k} (s', \pi_k (s')) \\ =  \mathcal{R} (s,a) + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P} (s,a,s') lim_{k \rightarrow \infty} max_{a \in \mathcal{A}} Q_{\pi_k} (s',a)) = \mathcal{R} (s,a) + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P} (s,a,s') max_{a \in \mathcal{A}} Q(s,a) $$
  
  Those equations show that $Q$ is a fixed-point of the Bellman Optimality Equation. However, we saw above that there only exists one unique fixed-point, i.e. $Q=Q^{\ast}$. As a consequence, acting greedily wrt $Q$ yields an optimal policy (see last post). Hopefully, those arguments give you a got feeling about why policy iteration succeeds in converging to our desired solution.

### Further Remarks

The ideas behind the Bellman Equations, Value and Policy iteration are crucial for RL solution methods. However, in most cases the precise algorithms described above are hardly applicable in practice. Reasons for this are primarily the size of the given MDP (e.g. can't evaluate policy for every state, or state-action pair) and/or the fact that we don't really understand the dynamics of the given MDP (e.g. don't fully understand transition function, only have samples). Besides, in RL applications the mathematical convergence for $k \rightarrow \infty$ is not really interesting; it will be more important to show that after a certain number of iteration steps the calculated value function or policy is near-optimal. In the next blog posts, we will discuss the famous Q-learning algorithm and introduce Deep Q-Networks.

> Conclusion: Every value function can be written in a recursive manner, which yields the different Bellman Equations. Those Bellman Equations can be transformed into update rules and finally, provide iterative algorithms that converge to the different value functions. Afterwards we deduced policy iteration, which is an algorithm that alternates between policy evaluation and policy improvement with the goal of solving a known MDP.    
    

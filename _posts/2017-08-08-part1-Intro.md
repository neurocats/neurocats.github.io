---
layout: post
comments: true
title:  "Introduction to Reinforcement Learning: Part 1 - the Framework"
excerpt: "Felix Assion - We will discuss the mathematical framework for RL problems - namely, Markov Decision Processes. The concept of value functions and action-value functions will be introduced. This series of posts is basically a brief summary of a workshop I held at the startup neurocat. "
date:   2017-08-08
mathjax: true
---

## Reinforcement Learning 

> "Good and evil, reward and punishment, are the only motives to a rational creature: these are the spur and reins whereby all mankind are set on work, and guided." (John Locke)


This exciting part of Machine Learning deals with agents that have to take 
actions (a sequence of actions) in a possibly unknown environment. Those agents try to maximize their personal utility (reward) by following a learned strategy (policy). This series of posts briefly summarizes my first experiences with Reinforcement Learning (RL). The overall goal of this series is to sketch all concepts needed to understand the paper "Playing Atari with Deep Reinforcement Learning" (by V. Mnih et al.). In the end, the presented theoretical models will be used to create an agent that can play my first and still favourite mobile phone game "Snake". I hope you enjoy the posts!       

### Markov Decision Processes

The majority of RL problems can be formalized using Markov Decision Processes (MDP). A MDP is a tuple $<\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma>$: 

*State Space* $\mathcal{S}$: The state space is the set that contains all possible environmental states. We will always assume that $\mathcal{S}$ is a countable set. Every state $s \in \mathcal{S}$ fully characterizes the given situation, hence it contains all relevant information from the history. In other words, we consider processes that have the Markov Property, i.e. "the future is independent of the past given the present". To illustrate this, let us quickly apply this to our Snake example: a state of the game is defined by the coordinates of the body of the snake, the coordinate point of the food and the direction of our current movement.   

*Action Space* $\mathcal{A}$: The action space is the set of possible actions, which can be taken in every state of the state space. We will always assume that $\mathcal{A}$ is a countable set. With those actions the agent tries to control the system state, thus a chosen action (usually) affects the next state of the given RL problem. For example, in every state of the game our snake has the option to go left, right, up or down. In this case, our decision in one state fully determines the following state of the game.

*Transition Function* $\mathcal{P}$: Let us assume that at time $t$ we experience the state $s_t \in \mathcal{S}$ and decide to take action $a_t \in \mathcal{A}$ (note: we will always assume discrete time steps in this workshop). Now, the system makes a transition from $s_t$ to a new state $s_{t+1} \in \mathcal{S}$ - this transition is determined by the transition function $\mathcal{P}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \[0,1 \]$. This function assigns to every tuple $<s_t, a_t, s>$  $\in \mathcal{S} \times \mathcal{A} \times \mathcal{S}$ the probability of ending up in state $s$ after taking action $a_t$ in state $s_t$. In other words, $\mathcal{P} (s_t,a_t,s) = \mathbb{P}\[s \mid s_t, a_t \]$ and $ \sum_{s \in \mathcal{S}} \mathcal{P} (s_t,a_t,s) = 1 $. 
In the case of "Snake" this transition function is quite simple, since we are in a very deterministic setting. Here every state-action pair fully determines the following state, i.e. for every $<s,a>$ $\in \mathcal{S} \times \mathcal{A}$  $\exists!$  $s' \in \mathcal{S}$ s.t. $\mathcal{P}(s,a,s')=1$.   

*Reward Function* $\mathcal{R}$: The (deterministic) reward function $\mathcal{R}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ tells us the immediate reward for some given state-action pair $<s_t, a_t>$. If it is clear from the context which state-action pair led to the immediate reward at time $t$, we will often abbreviate $R_t := \mathcal{R} (s_t,a_t)$. The concept of reward and punishment, represented by the reward function, is crucial in the world of RL. We implicitly always assume that gathering as much reward as possible is the core driver behind any rational decision. If we try to formalize our snake game, we could define the reward function as the change of the game score. This would imply that the snake receives a positive reward whenever it finds food. Additionally, we could think about punishing the snake whenever it bites its own body, i.e. punish the snake whenever it terminates the game.

*Discount Factor* $\gamma$: The discount factor $\gamma \in \[0,1 \]$ indicates how much the agent values immediate reward above delayed reward. To see this we need to define the return $G :=  \sum_{t=0}^\infty \gamma^t R_t$, i.e. the return is the discounted sum of immediate rewards collected throughout the life of the agent (note: if the agent's life terminates at time $T$, simply set $R_{t}=0$ for $\forall t > T$). Maximizing the expected return can be seen as the key goal of RL. First of all, the discount factor helps us to ensure that the infinite sum does not tend to infinity. Besides, it becomes clear, that if $\gamma$ is close to $0$ the agent basically only cares about the immediate reward of the next time step ("myopic behavior"). On the other hand, if $\gamma$ is close to $1$, the agent is very far-sighted and values immediate reward as much as future reward. In a financial setting we could interpret a discount factor $\gamma < 1$ as the existence of opportunity costs. However, in games we often set $\gamma = 1$, since games usually terminate after a finite number of time steps and we often don't care about "when" rewards appear - as long as the total score is high in the end. 

Throughout the rest of the series we will almost always assume that $\gamma < 1$ and that, at every point in time $t$, the function $\mathcal{R}$ is bounded by some $M \in \mathbb{R}$, i.e. $ \mid \mathcal{R} (s,a) \mid < M $ for $\forall s \in \mathcal{S}, \forall \in \mathcal{A}$. Those assumptions have some nice technical consequences: first of all, this guarantees the convergence of every sampled return series $G =  \sum_{t=0}^\infty \gamma^t R_t < \sum_{t=0}^\infty \gamma^t \mid R_t \mid < M (1 - \gamma)^{-1}$ (geometric series). Besides, this implies that any sampled return series converges absolutely, which allows us to apply the dominated convergence theorem (interchange sum with integral). This will be useful in the context of value functions.
          
          
### Policies and Value Functions

Having defined the right framework for our RL problem, we can now think about how to solve the MDP. As mentioned before, the goal will be to find a strategy that can maximize the expected return in every possible state of the state space. In the RL language a "strategy" of an agent is called a policy. To be more precise: A (stationary, deterministic) policy $\pi$ is a function that outputs for each state $s \in \mathcal{S}$ an action $a \in \mathcal{A}$, i.e. $\pi: \mathcal{S} \rightarrow \mathcal{A}$. Thus a policy fully defines the behaviour of an agent. Let $\Pi$ denote the set of all possible (deterministic) policies (note: we focus on deterministic policies, but in a lot of settings it might also make sense to consider stochastic policies $\pi: \mathcal{S} \times \mathcal{A} \rightarrow \[0,1\]$, e.g. if our agent is a robot and the joints can not be controlled "perfectly"). To be able to compare policies we define value functions. The state-value function $V_{\pi}: \mathcal{S} \rightarrow \mathbb{R}$ of the policy $\pi \in \Pi$ is defined by 

$$ V_{\pi} (s) := \mathbb{E} _{\pi} [ \sum _{t=0}^{\infty} \gamma ^t R_t \mid s ] $$

with $s \in \mathcal{S}$ (note: implicitly we always set the current state $s$ equal to $s_{0}$). Thus $V_{\pi} (s)$ is the expected value (return), when we are in state $s$ and are going to follow policy $\pi$. 

*Technical Remark* :It is not obvious how the integral (expected value) in the definition of the value function is defined. Here, one has to equip $\Omega := ( \mathcal{S} \times \mathcal{A})^{\infty} $ (infinite Cartesian product) with the infinite product topology and assign the Borel $\sigma$ -algebra. Additionally define the probability measure $\mathcal{P}_{\pi} ( (s_0,\pi(s_0),s_1,\pi(s_1),...)) := \mathcal{P}_0 (s_0) \mathcal{P} (s_0,\pi(s_0),s_1) \mathcal{P} (s_1,\pi(s_1),s_2)...$ for some initial distribution $\mathcal{P}_0$. Here we assume a deterministic policy and we implicitly set the probability of all trajectories, which are not possible under $\pi$, to zero.  
The explicit form of $\mathcal{P} _0$ is not important, since we almost always consider conditional expectancies. In most applications, one simply assumes that $\mathcal{P} _0$ is degenerate.

This definition also gives us a very intuitive partial ordering (reflexivity, antisymmetry and transitivity) on the set $\Pi$, namely $\pi \geq \pi'$ iff $V_{\pi}(s) \geq V_{\pi'}(s)$ for $\forall s \in \mathcal{S}$. Now, one fully understands what it means for a policy to be "better" than another policy. 
And solving a MDP can be boiled down to a search for an optimal policy $\pi ^{\ast}$ (not necessarily unique) with $V_{\pi ^{\ast}} (s)=sup_{\pi \in \Pi} V_{\pi} (s)$ for $\forall s \in \mathcal{S}$. It should be noted that the existence of an optimal policy is not directly clear - actually, there exist MDPs, which do not have an optimal policy:

*Example*: Let us consider the MDP, where the state space consists of one state $\mathcal{S} = < s >$ and the action space is equivalent to the natural numbers $\mathcal{A} = \mathbb{N} = < 1,2,3,... >$. 
Besides, the reward function is given by the deterministic function $\mathcal{R} (s,a) = 1 - a^{-1}$ and we have a discount factor $\gamma < 1$. In this case, the particular choice of the state space determines the transition function, since every action has to take us to the state $s$ (no other option) - i.e. $\mathcal{P} (s,a,s) = 1$ for $\forall a \in \mathcal{A}$. Altogether, this gives us a (infinite-horizon discounted) MDP. A policy $\pi \in \Pi$ simply chooses one action $a_{\pi} \in \mathcal{A}$, thus $\Pi \cong \mathbb{N}$. Thus, for a policy $\pi \in \Pi$ with $\pi (s) = a_{\pi}$ one has:

 $$ V_{\pi} (s) := \mathbb{E} _{\pi} [ \sum _{t=0}^{\infty} \gamma ^t R_t \mid s ] = \sum _{t=0}^{\infty} \gamma ^t \mathcal{R} (s,a _{\pi} ) = \sum _{t=0}^{\infty} \gamma ^t ( 1 - a _{\pi}^{-1} ) = ( 1 - \gamma) ^{-1} ( 1 - a _{\pi}^{-1} ) $$       

This implies that $sup_{\pi \in \Pi} V_{\pi} (s)= sup_{\pi \in \Pi} ( 1 - \gamma) ^{-1} ( 1 - a _{\pi}^{-1} ) = sup _{n \in \mathbb{N} } ( 1 - \gamma) ^{-1} (1 - n^{-1} ) = ( 1 - \gamma) ^{-1} $. However, there obviously doesn't exist any policy that has this value (due to the above calculation of the state-value functions), i.e. there doesn't exist any optimal policy in this MDP. 

Additionally one wants to define the action-value function $Q_{\pi}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ of policy $\pi$ by  

$$ Q_{\pi} (s,a) := \mathbb{E} _{\pi} [ \sum _{t=0}^{\infty} \gamma ^t R_t \mid s , a ] $$

with $s \in \mathcal{S}, a \in \mathcal{A}$. Hence, the action-value $Q_{\pi} (s,a)$ is the expected return starting from state $s$, taking action $a$ and then following policy $\pi$. Obviously, the state-value function and the action-value function are closely connected, which can also be stressed by the following simple equations:

$$ V_{\pi} (s) := \mathbb{E} _{\pi} [ \sum _{t=0}^{\infty} \gamma ^t R_t \mid s ] = \mathbb{E} _{\pi} [ \sum _{t=0}^{\infty} \gamma ^t R_t \mid s , \pi (s) ] = Q _{\pi} ( s, \pi (s)) $$ 

Independently of the existence of an optimal policy, we can define the optimal state-value function $V^{\ast}: \mathcal{S} \rightarrow \mathbb{R}$ and the optimal action-value function $Q^{\ast}: \mathcal{S} x \mathcal{A} \rightarrow \mathbb{R}$ 

 $$ V^{\ast} (s) := sup_{ \pi \in \Pi} V_{\pi} (s)  $$
 
 $$ Q^{\ast} (s,a) := sup_{ \pi \in \Pi} Q_{\pi} (s,a) = \mathcal{R} (s,a) + \gamma \sum_{ y \in \mathcal{S} } \mathcal{P} (s,a,y) V^{\ast} (y) $$ 

$\forall s \in \mathcal{S} , \forall a \in \mathcal{A}$. It also follows that $V^{\ast} (s) = sup_{a \in \mathcal{A}} Q^{\ast} (s,a)$. 
Another import term in the language of RL is "greedy"; an agent acts greedily if it takes locally optimal choices with the hope of finding a global optimum. To be more precise: The deterministic policy $\pi'$ is greedy with respect to $Q_{\pi}$ with $\pi \in \Pi$ if $\pi'(s) \in argmax_{a \in \mathcal{A}} Q_{\pi} (s, a)$ for all $s \in \mathcal{S}$.

Let us assume that there exists a policy $\pi'$ that acts greedily with respect to the optimal action-value function $Q^{\ast}$ (e.g. possible if the action space is finite):

$$ V^{\ast} (s) = sup_{a \in \mathcal{A}} Q^{\ast} (s,a) = Q^{\ast} (s,\pi'(s))= \mathcal{R} (s,\pi'(s)) + \gamma \sum_{ y \in \mathcal{S} } \mathcal{P} (s,a,y) V^{\ast} (y) \\  = \mathcal{R} (s,\pi'(s)) + \gamma \sum_{ y \in \mathcal{S} } \mathcal{P} (s,a,y) Q^{\ast} (y,\pi'(y) ) = ... = V_{\pi'} (s) $$

$\forall s \in \mathcal{S}$. Thus, acting greedily wrt the optimal action-value function gives us an optimal policy (solution to the MDP -> existence of optimal policy (!)). This justifies the definition of action-value functions in general, because we now have a reasonable solution strategy for a MDP.
However, one can only develop this greedy policy if the optimal action-value function $Q ^{\ast}$ is known. Even in a known MDP, where the transition and reward dynamics are clear, it might be hard to calculate the optimal action-value function (or in general: any value function) directly. A relatively large MDP, i.e. with a lot of states and action, will soon make it impossible to calculate the expected values (integrals) of the value functions without any iterative algorithm. 

Therefore, the next post provides iterative algorithms that converge to the desired value functions.  

> Conclusion: Markov Decision Processes (MDPs) provide a suitable mathematical framework for Reinforcement Learning (RL) problems. In order to find an optimal policy one makes use of (optimal) state-value and (optimal) action-value functions.

  

